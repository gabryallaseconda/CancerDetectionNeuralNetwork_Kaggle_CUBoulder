{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Training process of the final model with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tools import *\n",
    "\n",
    "def write_log(logfile, train_loss, test_loss, test_score, lr):\n",
    "    with open(logfile, \"a+\") as log:\n",
    "        log.write(\"{}\\t{}\\t{}\\t{}\\n\".format(train_loss, test_loss, test_score, lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting DEVICE:\n",
      "\t MPS is available\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing label dataset\n",
    "\n",
    "To run the full crossvalidation, remove .head(30000). This will multiply about by 20 the time required by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FOLDERS PATH\n",
    "source_dir = 'histopathologic-cancer-detection/'\n",
    "train_im_source_dir = source_dir+'/train'\n",
    "\n",
    "# IMPORTING DATA\n",
    "train_data = pd.read_csv(os.path.join(source_dir,'train_labels.csv')) \n",
    "train_data = train_data.sample(frac=1, random_state=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ids = list(train_data.id)\n",
    "data_labels = list(train_data.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging directories\n",
    "model_source_dir = 'trained_model/'\n",
    "model_name = 'final'\n",
    "\n",
    "# Batch size\n",
    "b_size = int(96) # batch size\n",
    "\n",
    "# Epochs\n",
    "n_epochs = 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielegabrielli/Library/Caches/pypoetry/virtualenvs/neural-networks-_F4AaA2c-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gabrielegabrielli/Library/Caches/pypoetry/virtualenvs/neural-networks-_F4AaA2c-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0] \tLoss: 0.711788\n",
      "Train Epoch: 0 [9600] \tLoss: 0.575053\n",
      "Train Epoch: 0 [19200] \tLoss: 0.516960\n",
      "Train Epoch: 0 [28800] \tLoss: 0.508497\n",
      "Train Epoch: 0 [38400] \tLoss: 0.508654\n",
      "Train Epoch: 0 [48000] \tLoss: 0.495767\n",
      "Train Epoch: 0 [57600] \tLoss: 0.499734\n",
      "Train Epoch: 0 [67200] \tLoss: 0.491936\n",
      "Train Epoch: 0 [76800] \tLoss: 0.502565\n",
      "Train Epoch: 0 [86400] \tLoss: 0.492402\n",
      "Train Epoch: 0 [96000] \tLoss: 0.495736\n",
      "Train Epoch: 0 [105600] \tLoss: 0.494938\n",
      "Train Epoch: 0 [115200] \tLoss: 0.490527\n",
      "Train Epoch: 0 [124800] \tLoss: 0.491509\n",
      "Train Epoch: 0 [134400] \tLoss: 0.487060\n",
      "Train Epoch: 0 [144000] \tLoss: 0.500588\n",
      "Train Epoch: 0 [153600] \tLoss: 0.493360\n",
      "Train Epoch: 0 [163200] \tLoss: 0.486640\n",
      "Train Epoch: 0 [172800] \tLoss: 0.494440\n",
      "Train Epoch: 0 [182400] \tLoss: 0.490889\n",
      "Train Epoch: 0 [192000] \tLoss: 0.493751\n",
      "Train Epoch: 0 [201600] \tLoss: 0.490738\n",
      "Train Epoch: 0 [211200] \tLoss: 0.477821\n",
      "Mean train loss on epoch 0 : 0.5083197356695714\n",
      "Begin of epoch 0\n",
      "Train Epoch: 0 [0] \tLoss: 0.491914\n",
      "Train Epoch: 0 [96000] \tLoss: 0.251667\n",
      "Train Epoch: 0 [192000] \tLoss: 0.189436\n",
      "Mean train loss on epoch 0 : 0.3110057387252649\n",
      "\t end epoch - loss: 0.311006\n",
      "Begin of epoch 1\n",
      "Train Epoch: 1 [0] \tLoss: 0.119372\n",
      "Train Epoch: 1 [96000] \tLoss: 0.158322\n",
      "Train Epoch: 1 [192000] \tLoss: 0.152306\n",
      "Mean train loss on epoch 1 : 0.14333305093149343\n",
      "\t end epoch - loss: 0.143333\n",
      "Begin of epoch 2\n",
      "Train Epoch: 2 [0] \tLoss: 0.109793\n",
      "Train Epoch: 2 [96000] \tLoss: 0.139253\n",
      "Train Epoch: 2 [192000] \tLoss: 0.137115\n",
      "Mean train loss on epoch 2 : 0.12872007862788937\n",
      "\t end epoch - loss: 0.128720\n",
      "Begin of epoch 3\n",
      "Train Epoch: 3 [0] \tLoss: 0.104335\n",
      "Train Epoch: 3 [96000] \tLoss: 0.124301\n",
      "Train Epoch: 3 [192000] \tLoss: 0.125509\n",
      "Mean train loss on epoch 3 : 0.11804843034967778\n",
      "\t end epoch - loss: 0.118048\n",
      "Begin of epoch 4\n",
      "Train Epoch: 4 [0] \tLoss: 0.085318\n",
      "Train Epoch: 4 [96000] \tLoss: 0.115582\n",
      "Train Epoch: 4 [192000] \tLoss: 0.116199\n",
      "Mean train loss on epoch 4 : 0.10569961341284216\n",
      "\t end epoch - loss: 0.105700\n",
      "Begin of epoch 5\n",
      "Train Epoch: 5 [0] \tLoss: 0.058205\n",
      "Train Epoch: 5 [96000] \tLoss: 0.107561\n",
      "Train Epoch: 5 [192000] \tLoss: 0.108247\n",
      "Mean train loss on epoch 5 : 0.09133741056608657\n",
      "\t end epoch - loss: 0.091337\n",
      "Begin of epoch 6\n",
      "Train Epoch: 6 [0] \tLoss: 0.068157\n",
      "Train Epoch: 6 [96000] \tLoss: 0.101035\n",
      "Train Epoch: 6 [192000] \tLoss: 0.102484\n",
      "Mean train loss on epoch 6 : 0.09055912977084518\n",
      "\t end epoch - loss: 0.090559\n",
      "Begin of epoch 7\n",
      "Train Epoch: 7 [0] \tLoss: 0.095151\n",
      "Train Epoch: 7 [96000] \tLoss: 0.095503\n",
      "Train Epoch: 7 [192000] \tLoss: 0.096863\n",
      "Mean train loss on epoch 7 : 0.0958391365405793\n",
      "\t end epoch - loss: 0.095839\n",
      "Begin of epoch 8\n",
      "Train Epoch: 8 [0] \tLoss: 0.091881\n",
      "Train Epoch: 8 [96000] \tLoss: 0.091857\n",
      "Train Epoch: 8 [192000] \tLoss: 0.091999\n",
      "Mean train loss on epoch 8 : 0.0919125910246124\n",
      "\t end epoch - loss: 0.091913\n",
      "Begin of epoch 9\n",
      "Train Epoch: 9 [0] \tLoss: 0.082949\n",
      "Train Epoch: 9 [96000] \tLoss: 0.088822\n",
      "Train Epoch: 9 [192000] \tLoss: 0.088849\n",
      "Mean train loss on epoch 9 : 0.08687319036697348\n",
      "\t end epoch - loss: 0.086873\n",
      "Begin of epoch 10\n",
      "Train Epoch: 10 [0] \tLoss: 0.092592\n",
      "Train Epoch: 10 [96000] \tLoss: 0.083502\n",
      "Train Epoch: 10 [192000] \tLoss: 0.084587\n",
      "Mean train loss on epoch 10 : 0.08689368371727564\n",
      "\t end epoch - loss: 0.086894\n",
      "Begin of epoch 11\n",
      "Train Epoch: 11 [0] \tLoss: 0.046678\n",
      "Train Epoch: 11 [96000] \tLoss: 0.080818\n",
      "Train Epoch: 11 [192000] \tLoss: 0.081749\n",
      "Mean train loss on epoch 11 : 0.06974827487797787\n",
      "\t end epoch - loss: 0.069748\n",
      "Begin of epoch 12\n",
      "Train Epoch: 12 [0] \tLoss: 0.098899\n",
      "Train Epoch: 12 [96000] \tLoss: 0.078015\n",
      "Train Epoch: 12 [192000] \tLoss: 0.077100\n",
      "Mean train loss on epoch 12 : 0.08467139535521467\n",
      "\t end epoch - loss: 0.084671\n",
      "Begin of epoch 13\n",
      "Train Epoch: 13 [0] \tLoss: 0.054899\n",
      "Train Epoch: 13 [96000] \tLoss: 0.074059\n",
      "Train Epoch: 13 [192000] \tLoss: 0.074760\n",
      "Mean train loss on epoch 13 : 0.0679060962962297\n",
      "\t end epoch - loss: 0.067906\n",
      "Begin of epoch 14\n",
      "Train Epoch: 14 [0] \tLoss: 0.056116\n",
      "Train Epoch: 14 [96000] \tLoss: 0.072175\n",
      "Train Epoch: 14 [192000] \tLoss: 0.073043\n",
      "Mean train loss on epoch 14 : 0.06711144917939478\n",
      "\t end epoch - loss: 0.067111\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setting log files\n",
    "logfile =  model_source_dir+'/{}.logfile.txt'.format(model_name)\n",
    "best_w_path = model_source_dir+'/{}.best.pt'.format(model_name)\n",
    "model_path =  model_source_dir+'/{}.model.pt'.format(model_name)\n",
    "    \n",
    "# Data augmentation functions\n",
    "training_aug = aug_train() \n",
    "    \n",
    "# Learning Rate setting. This will be modified according to cyclic scheduler\n",
    "curr_lr = 3e-3 \n",
    "    \n",
    "# Loader for the training and the validation\n",
    "train_loader = torch.utils.data.DataLoader(DataGenerator(\n",
    "                                                data_ids,\n",
    "                                                data_labels,\n",
    "                                                training_aug, \n",
    "                                                train_im_source_dir),\n",
    "                                            pin_memory=False,\n",
    "                                            num_workers=4,\n",
    "                                            batch_size=b_size) \n",
    "                                            \n",
    "# Loss function \n",
    "loss_f = nn.BCELoss() # BINARY CROSS ENTROPY\n",
    "\n",
    "# Import pretrained model\n",
    "base_model = pretrainedmodels.resnet34(num_classes=1000, \n",
    "                                           pretrained='imagenet').to(device) \n",
    "\n",
    "    \n",
    "# Shape the model    \n",
    "model = Net(base_model, 512).to(device)\n",
    "\n",
    "# Optimizer\n",
    "# Some layers are freezed for the first iteration, by setting the learning rate to zero\n",
    "optimizer = optim.SGD([{'params': model.layer0.parameters(), 'lr': 0},\n",
    "                        {'params': model.layer2.parameters(), 'lr': 0},\n",
    "                        {'params': model.layer1.parameters(), 'lr': 0},\n",
    "                        {'params': model.layer3.parameters(), 'lr': 0},\n",
    "                        {'params': model.layer4.parameters(), 'lr': 0},\n",
    "                        {'params': model.classif.parameters()}], lr=0.05, momentum=0.9)\n",
    "    \n",
    "# First Training procedure\n",
    "train_loss = train(model= model,\n",
    "                    train_loader= train_loader, \n",
    "                    optimizer= optimizer, \n",
    "                    epoch= 0, \n",
    "                    log_interval= 100, \n",
    "                    loss_f= loss_f, \n",
    "                    scheduler= None,\n",
    "                    device=device)\n",
    "    \n",
    "    \n",
    "# Loop on epochs\n",
    "\n",
    "# Values to monitor the loss trough the loop on epochs\n",
    "best_score = 0\n",
    "best_loss = 1e5\n",
    "idx_stop = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Print current epoch\n",
    "    print(f'Begin of epoch {epoch}')\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=curr_lr, momentum=0.9)\n",
    "\n",
    "    # Scheduler for triangular cyclic learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=curr_lr, max_lr=3*curr_lr, mode = 'triangular')\n",
    "\n",
    "    # Train procedure\n",
    "    train_loss = train(model= model,\n",
    "                    train_loader= train_loader, \n",
    "                    optimizer= optimizer, \n",
    "                    epoch= epoch, \n",
    "                    log_interval= 1000, \n",
    "                    loss_f= loss_f, \n",
    "                    scheduler= scheduler,\n",
    "                    device = device)\n",
    "    \n",
    "    print('\\t end epoch - loss: {:.6f}'.format(train_loss))\n",
    "        \n",
    "        \n",
    "    \n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-networks-_F4AaA2c-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
