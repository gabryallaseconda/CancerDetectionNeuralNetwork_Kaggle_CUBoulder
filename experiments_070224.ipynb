{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    130908\n",
       "1     89117\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"histopathologic-cancer-detection/train_labels.csv\")\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.468945319074924"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "130908/89117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset has 220025 rows\n",
      "Splitted has in total 220025 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def split_dataframe(df:pd.DataFrame, num_of_splits:int) -> list[pd.DataFrame]:\n",
    "\n",
    "    # Shuffle the rows\n",
    "    df_shuffled = df.sample(frac=1, random_state=10) \n",
    "\n",
    "    rows_per_split = len(df_shuffled) // num_of_splits\n",
    "\n",
    "    dfs = [df_shuffled.iloc[i * rows_per_split:(i + 1) * rows_per_split] for i in range(num_of_splits)]\n",
    "\n",
    "    # Print to check line losses\n",
    "    print(f'Original dataset has {len(df)} rows')\n",
    "    print(f'Splitted has in total {sum([len(part) for part in dfs])} rows')\n",
    "\n",
    "    return dfs\n",
    "\n",
    "dfs = split_dataframe(df = df, num_of_splits = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torchvision import datasets, transforms\n",
    "#from torch.autograd import Variable\n",
    "#from torch.optim import Optimizer\n",
    "#from torch.utils import data\n",
    "import pretrainedmodels\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "#import cv2\n",
    "from skimage.io import imread\n",
    "#from torch.utils.data.sampler import WeightedRandomSampler, BatchSampler\n",
    "#from tqdm import tqdm\n",
    "#from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold\n",
    "#import pretrainedmodels.utils as utils\n",
    "#from sklearn.metrics import roc_auc_score\n",
    "from tools import *\n",
    "\n",
    "def write_log(logfile, train_loss, test_loss, test_score, lr):\n",
    "    with open(logfile, \"a+\") as log:\n",
    "        log.write(\"{}\\t{}\\t{}\\t{}\\n\".format(train_loss, test_loss, test_score, lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PER IMPOSTARE IL DEVICE\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('MPS is available')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('No acceleration available')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f38a6374c348f90b587e046aac6079959adf3835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c18f2d887b7ae4f6742ee445113fa1aef383ed77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>755db6279dae599ebb4d39a9123cce439965282d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bc3f0c64fb968ff4a8bd33af6971ecae77c75e08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>068aba587a4950175d04c680d38943fd488d6a9d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         id  label\n",
       "0  f38a6374c348f90b587e046aac6079959adf3835      0\n",
       "1  c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n",
       "2  755db6279dae599ebb4d39a9123cce439965282d      0\n",
       "3  bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n",
       "4  068aba587a4950175d04c680d38943fd488d6a9d      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# FOLDERS PATH\n",
    "DIR = 'histopathologic-cancer-detection/'\n",
    "train_im_dir = DIR+'/train'\n",
    "test_im_dir = DIR+'/test'\n",
    "\n",
    "# IMPORTING DATA\n",
    "train_data = pd.read_csv(os.path.join(DIR,'train_labels.csv')) #labels for train data\n",
    "\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELS\n",
    "# model_dir = os.path.join(DIR, 'resnet34')\n",
    "# model_name = 'resnet34'\n",
    "model_dir = os.path.join(DIR, 'this_model')\n",
    "model_name = 'this_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_groups = 15 # number of folds # CROSS-VALIDATION PER VALIDAZIONE\n",
    "b_size = 96 # batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS VALIDATION K-FOLD \n",
    "skf = StratifiedKFold(n_splits=n_groups)\n",
    "\n",
    "# CONTENITORI PER RACCOGLIERE I DATI DELLA CROSS-VALIDATION\n",
    "folds_id_train = []\n",
    "folds_label_train = []\n",
    "folds_id_val = []\n",
    "folds_label_val = []\n",
    "\n",
    "# RIEMPIO I CONTENITORI PER LA CROSS-VALIDATION\n",
    "for train_index, test_index in skf.split(train_data['id'].values, train_data['label'].values):\n",
    "    # Train\n",
    "    folds_id_train.append(train_data['id'].values[train_index])\n",
    "    folds_label_train.append(train_data['label'].values[train_index])\n",
    "    # Validation\n",
    "    folds_id_val.append(train_data['id'].values[test_index])    \n",
    "    folds_label_val.append(train_data['label'].values[test_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "205356\n",
      "15\n",
      "14669\n"
     ]
    }
   ],
   "source": [
    "print(len(folds_id_train))\n",
    "print(len(folds_id_train[0]))\n",
    "\n",
    "print(len(folds_id_val))\n",
    "print(len(folds_id_val[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielegabrielli/Library/Caches/pypoetry/virtualenvs/neural-networks-_F4AaA2c-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gabrielegabrielli/Library/Caches/pypoetry/virtualenvs/neural-networks-_F4AaA2c-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Train Epoch: 0 [0/50000 (0.000%)]\tLoss: 0.704052\n",
      "Train Epoch: 0 [9600/50000 (19.200%)]\tLoss: 0.704023\n",
      "Train Epoch: 0 [19200/50000 (38.400%)]\tLoss: 0.702323\n",
      "Train Epoch: 0 [28800/50000 (57.600%)]\tLoss: 0.704685\n",
      "Train Epoch: 0 [38400/50000 (76.800%)]\tLoss: 0.702748\n",
      "Train Epoch: 0 [48000/50000 (96.000%)]\tLoss: 0.703608\n",
      "Mean train loss on epoch 0 : 0.7035729429125785\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "test() got an unexpected keyword argument 'val_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 86\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# QUI INIZIA GIà A TRAINARE, QUESTO è IL PRIMO GIRO, CHISSà PERCHè LO FA ESTERNO\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# LAUNCH TRAIN, TEST AND WRITE LOG\u001b[39;00m\n\u001b[1;32m     77\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train(model\u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m     78\u001b[0m                        train_loader\u001b[38;5;241m=\u001b[39m train_loader, \n\u001b[1;32m     79\u001b[0m                        optimizer\u001b[38;5;241m=\u001b[39m optimizer, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m                        samples_per_epoch\u001b[38;5;241m=\u001b[39m samples_per_epoch,\n\u001b[1;32m     84\u001b[0m                        scheduler\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 86\u001b[0m test_loss, score \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mloss_f\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_f\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m write_log(logfile, train_loss, test_loss, score, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mstart training the model with all layers\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03mTraining scheme : train while validation loss decreases, save model at each improvement of test loss. \u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mif loss does not decreases for 3 epochs, reload last best model, reduce lr by factor of 2. \u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03mIf loss still doesn't decrease for 10 epochs, stop the model. \u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: test() got an unexpected keyword argument 'val_loader'"
     ]
    }
   ],
   "source": [
    "\n",
    "# INIZIO DEL TRAINING\n",
    "samples_per_epoch = 50000 #define number of samples per epoch, since dataset is big\n",
    "# CICLO SUI GRUPPI DELLA C.V.\n",
    "for valid_idx in range(n_groups):\n",
    "\n",
    "    logfile =  model_dir+'/{}.fold{}.logfile.txt'.format(model_name, valid_idx)\n",
    "    best_w_path = model_dir+'/{}.fold{}.best.pt'.format(model_name, valid_idx)\n",
    "    es_w_path =  model_dir+'/{}.fold{}.es.pt'.format(model_name, valid_idx)\n",
    "    \n",
    "    \n",
    "    print('Training fold {}'.format(valid_idx))\n",
    "    \n",
    "    #with open(logfile, \"w\") as log:\n",
    "    #    pass    \n",
    "    \n",
    "    traing_aug = aug_train() # FUNZIONE IN UTILS\n",
    "    validation_aug = aug_val() # FUNZIONE IN UTILS\n",
    "    \n",
    "    curr_lr = 3e-4\n",
    "    \n",
    "    train_sampler = torch.utils.data.RandomSampler(DataGenerator(folds_id_val[valid_idx],       # GENERATES DATASET FOR LOADING\n",
    "                                                                 folds_label_val[valid_idx], \n",
    "                                                                 validation_aug, train_im_dir),\n",
    "                                                   replacement=True, \n",
    "                                                   num_samples=samples_per_epoch)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(DataGenerator(folds_id_train[valid_idx], \n",
    "                                                             folds_label_train[valid_idx], \n",
    "                                                             traing_aug, train_im_dir),\n",
    "                                               pin_memory=False,\n",
    "                                               num_workers=4,\n",
    "                                               batch_size=b_size, \n",
    "                                               sampler=train_sampler)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(DataGenerator(folds_id_val[valid_idx], \n",
    "                                                       folds_label_val[valid_idx], \n",
    "                                                       validation_aug, train_im_dir),\n",
    "                                             pin_memory=False,\n",
    "                                             num_workers=1,\n",
    "                                             batch_size=b_size)\n",
    "    \n",
    "    # DEFINIAMO LA FUNZIONE DI LOSS - \n",
    "    loss_f = nn.BCELoss() # BINARY CROSS ENTROPY\n",
    "\n",
    "    best_score = 0\n",
    "    best_loss = 1e5\n",
    "    idx_stop = 0\n",
    "\n",
    "    ##############################################\n",
    "    # OK QUI BISOGNA LAVORARCI\n",
    "\n",
    "    # LOAD RESNET34 - PRETRAINED  # questo andrebbe sostituito cazzo!\n",
    "    base_model = pretrainedmodels.resnet34(num_classes=1000, \n",
    "                                           pretrained='imagenet').to(device) \n",
    "    \n",
    "    #model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)  # Esempio di ResNet-18 preaddestrata\n",
    "\n",
    "    \n",
    "    # DEFINISCO IL MODEL! # OK QUI \n",
    "    model = Net(base_model, 512).to(device)\n",
    "\n",
    "    ##############################################\n",
    "    \n",
    "    # \"TRAINING WITH FROZEN LAYERS EXCEPT FOR CLASSIFICATION HEAD\"\n",
    "    # OK, ALCUNI LAYERS VENGONO BLOCCATI CON UN LEARNING RATE PARI A ZERO, PER CUI NON VENGONO RIADDESTRATI!\n",
    "    optimizer = optim.SGD([{'params': model.layer0.parameters(), 'lr': 0},\n",
    "                           {'params': model.layer1.parameters(), 'lr': 0},\n",
    "                           {'params': model.layer2.parameters(), 'lr': 0},\n",
    "                           {'params': model.layer3.parameters(), 'lr': 0},\n",
    "                           {'params': model.layer4.parameters(), 'lr': 0},\n",
    "                           {'params': model.classif.parameters()}], lr=0.05, momentum=0.9)\n",
    "    \n",
    "\n",
    "    \n",
    "    # QUI INIZIA GIà A TRAINARE, QUESTO è IL PRIMO GIRO, CHISSà PERCHè LO FA ESTERNO\n",
    "    # LAUNCH TRAIN, TEST AND WRITE LOG\n",
    "    train_loss = train(model= model,\n",
    "                           train_loader= train_loader, \n",
    "                           optimizer= optimizer, \n",
    "                           epoch= 0, \n",
    "                           log_interval= 100, \n",
    "                           loss_f= loss_f, \n",
    "                           samples_per_epoch= samples_per_epoch,\n",
    "                           scheduler= None)\n",
    "    \n",
    "    test_loss, score = test(model= model, \n",
    "                                val_loader= val_loader, \n",
    "                                loss_f= loss_f)\n",
    "    \n",
    "    write_log(logfile, train_loss, test_loss, score, lr = \"not available\")\n",
    "    \n",
    "    '''\n",
    "    start training the model with all layers\n",
    "    Training scheme : train while validation loss decreases, save model at each improvement of test loss. \n",
    "    if loss does not decreases for 3 epochs, reload last best model, reduce lr by factor of 2. \n",
    "    If loss still doesn't decrease for 10 epochs, stop the model. \n",
    "    '''\n",
    "    for epoch in range(50):\n",
    "        print(f'we are in epoch {epoch}')\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=curr_lr, momentum=0.9)\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=curr_lr, max_lr=3*curr_lr, mode = 'triangular')\n",
    "        #scheduler = CyclicLR(optimizer, max_lr=3*curr_lr)\n",
    "\n",
    "        # LAUNCH TRAIN, TEST AND WRITE LOG\n",
    "        train_loss = train(model= model,\n",
    "                           train_loader= train_loader, \n",
    "                           optimizer= optimizer, \n",
    "                           epoch= epoch, \n",
    "                           log_interval= 100, \n",
    "                           loss_f= loss_f, \n",
    "                           samples_per_epoch= samples_per_epoch,\n",
    "                           scheduler= scheduler)\n",
    "        \n",
    "        test_loss, score = test(model= model, \n",
    "                                test_loader= val_loader, \n",
    "                                loss_f= loss_f)\n",
    "        \n",
    "        write_log(logfile, train_loss, test_loss, score)\n",
    "        \n",
    "        if test_loss<best_loss:\n",
    "            print('Test loss improved from {} to {}, saving'.format(best_loss, test_loss))\n",
    "            best_loss = test_loss\n",
    "            torch.save(model.state_dict(), best_w_path)\n",
    "            idx_stop = 0\n",
    "        else:\n",
    "            print('Loss {}, did not improve from {} for {} epochs'.format(test_loss, best_loss, idx_stop))\n",
    "            idx_stop += 1\n",
    "        if idx_stop>3:\n",
    "            print('Reducing LR by two and reloading best model')\n",
    "            model.load_state_dict(torch.load(best_w_path))\n",
    "            curr_lr = curr_lr/2\n",
    "        if idx_stop>10:\n",
    "            print('Stopping the model')\n",
    "            torch.save(model.state_dict(), es_w_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-networks-_F4AaA2c-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
