{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pretrainedmodels\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from tools import *\n",
    "\n",
    "def write_log(logfile, train_loss, test_loss, test_score, lr):\n",
    "    with open(logfile, \"a+\") as log:\n",
    "        log.write(\"{}\\t{}\\t{}\\t{}\\n\".format(train_loss, test_loss, test_score, lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PER IMPOSTARE IL DEVICE\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('MPS is available')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('No acceleration available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing label dataset\n",
    "\n",
    "To run the full crossvalidation, remove .head(30000). This will multiply about by 20 the time required by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# FOLDERS PATH\n",
    "source_dir = 'histopathologic-cancer-detection/'\n",
    "train_im_source_dir = source_dir+'/train'\n",
    "test_im_source_dir = source_dir+'/test'\n",
    "\n",
    "# IMPORTING DATA\n",
    "train_data = pd.read_csv(os.path.join(source_dir,'train_labels.csv')) \n",
    "train_data = train_data.sample(frac=1, random_state=10).head(30000)\n",
    "train_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_samples(dataset, n, etichetta):\n",
    "    label_matching_indexes = dataset.index[dataset['label'] == etichetta].tolist()\n",
    "    label_matching_indexes = label_matching_indexes[:n]\n",
    "    return dataset.loc[label_matching_indexes], dataset.drop(index=label_matching_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_groups = 20 # number of folds # CROSS-VALIDATION PER VALIDAZIONE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validataion_proportion = 0.2\n",
    "\n",
    "# Deduce this proportion by the validation\n",
    "train_proportion = 1-validataion_proportion\n",
    "\n",
    "# Get number of elements for each class\n",
    "zeros = len(train_data[train_data.label == 0])\n",
    "ones = len(train_data[train_data.label == 1])\n",
    "\n",
    "# Get the proportion of elements for training, splitted for each class\n",
    "train_zeros = int(zeros/n_groups*0.9)\n",
    "train_ones = int(ones/n_groups*0.9)\n",
    "\n",
    "# Get the proportion of elements for training, splitted for each class\n",
    "val_zeros = int(zeros/n_groups*0.1)\n",
    "val_ones = int(ones/n_groups*0.1)\n",
    "\n",
    "# Shuffle the training data, get df dataset\n",
    "df = train_data.sample(frac=1, random_state=10)\n",
    "\n",
    "# Initializate empty lists\n",
    "folds_id_train = []\n",
    "folds_label_train = []\n",
    "folds_id_val = []\n",
    "folds_label_val = []\n",
    "\n",
    "# Loop on elements for validation\n",
    "for i in range(n_groups):\n",
    "\n",
    "    # Get first elements in df, return also df without elements\n",
    "    fold_train_zeros, df = get_samples(df, train_zeros, 0)\n",
    "    fold_train_ones, df = get_samples(df, train_ones, 1)\n",
    "\n",
    "    # Merge zero and ones datasets together\n",
    "    fold_train = pd.concat([fold_train_zeros, fold_train_ones], ignore_index=True).sample(frac=1, random_state=10+n_groups)\n",
    "\n",
    "    # Append id and label to the two lists\n",
    "    folds_id_train.append(fold_train['id'].values)\n",
    "    folds_label_train .append(fold_train['label'].values)\n",
    "\n",
    "    # Same but for validation\n",
    "    fold_val_zeros, df = get_samples(df, val_zeros, 0)\n",
    "    fold_val_ones, df = get_samples(df, val_ones, 1)\n",
    "\n",
    "    fold_val = pd.concat([fold_val_zeros, fold_val_ones], ignore_index=True).sample(frac=1, random_state=10+n_groups)\n",
    "\n",
    "    folds_id_val.append(fold_val['id'].values)    \n",
    "    folds_label_val.append(fold_val['label'].values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging directories\n",
    "model_source_dir = 'cross_validation_logs/'\n",
    "model_name = 'cv'\n",
    "\n",
    "# Batch size\n",
    "b_size = 96 # batch size\n",
    "\n",
    "# Epochs\n",
    "n_epochs = 15\n",
    "\n",
    "# Set samples per epoch\n",
    "#samples_per_epoch = 50000 \n",
    "\n",
    "# Save loss for each fold\n",
    "fold_loss = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - loop on cv folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielegabrielli/Library/Caches/pypoetry/virtualenvs/neural-networks-_F4AaA2c-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gabrielegabrielli/Library/Caches/pypoetry/virtualenvs/neural-networks-_F4AaA2c-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 0 : 0.6708741188049316\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.644994, roc auc: 0.7315\n",
      "\n",
      "we are in epoch 0\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 0 : 0.6235868334770203\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.510615, roc auc: 0.8854\n",
      "\n",
      "Test LOSS IMPROVED from 100000.0 to 0.5106145441532135, saving\n",
      "we are in epoch 1\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 1 : 0.5237208604812622\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.424325, roc auc: 0.9099\n",
      "\n",
      "Test LOSS IMPROVED from 0.5106145441532135 to 0.42432481050491333, saving\n",
      "we are in epoch 2\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 2 : 0.46702301502227783\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.335865, roc auc: 0.9200\n",
      "\n",
      "Test LOSS IMPROVED from 0.42432481050491333 to 0.3358645588159561, saving\n",
      "we are in epoch 3\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 3 : 0.44455718994140625\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.343708, roc auc: 0.9219\n",
      "\n",
      "Loss 0.34370845556259155, did NOT improve from 0.3358645588159561 for 0 epochs\n",
      "we are in epoch 4\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 4 : 0.3837915360927582\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.295117, roc auc: 0.9451\n",
      "\n",
      "Test LOSS IMPROVED from 0.3358645588159561 to 0.29511716961860657, saving\n",
      "we are in epoch 5\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 5 : 0.38841819763183594\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.286060, roc auc: 0.9301\n",
      "\n",
      "Test LOSS IMPROVED from 0.29511716961860657 to 0.2860601991415024, saving\n",
      "we are in epoch 6\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 6 : 0.33548110723495483\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.248972, roc auc: 0.9506\n",
      "\n",
      "Test LOSS IMPROVED from 0.2860601991415024 to 0.2489720806479454, saving\n",
      "we are in epoch 7\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 7 : 0.36078566312789917\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.289417, roc auc: 0.9429\n",
      "\n",
      "Loss 0.2894170433282852, did NOT improve from 0.2489720806479454 for 0 epochs\n",
      "we are in epoch 8\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 8 : 0.3164950907230377\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.263062, roc auc: 0.9466\n",
      "\n",
      "Loss 0.2630624696612358, did NOT improve from 0.2489720806479454 for 1 epochs\n",
      "we are in epoch 9\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 9 : 0.30663129687309265\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.231809, roc auc: 0.9588\n",
      "\n",
      "Test LOSS IMPROVED from 0.2489720806479454 to 0.23180866241455078, saving\n",
      "we are in epoch 10\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 10 : 0.30595067143440247\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.237973, roc auc: 0.9537\n",
      "\n",
      "Loss 0.23797325044870377, did NOT improve from 0.23180866241455078 for 0 epochs\n",
      "we are in epoch 11\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 11 : 0.3284127414226532\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.215638, roc auc: 0.9586\n",
      "\n",
      "Test LOSS IMPROVED from 0.23180866241455078 to 0.21563754975795746, saving\n",
      "we are in epoch 12\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 12 : 0.26761943101882935\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.225969, roc auc: 0.9594\n",
      "\n",
      "Loss 0.225969098508358, did NOT improve from 0.21563754975795746 for 0 epochs\n",
      "we are in epoch 13\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 13 : 0.2325090765953064\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.294929, roc auc: 0.9530\n",
      "\n",
      "Loss 0.294928677380085, did NOT improve from 0.21563754975795746 for 1 epochs\n",
      "we are in epoch 14\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 14 : 0.2165977656841278\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.222322, roc auc: 0.9616\n",
      "\n",
      "Loss 0.22232206165790558, did NOT improve from 0.21563754975795746 for 2 epochs\n",
      "Training fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielegabrielli/Library/Caches/pypoetry/virtualenvs/neural-networks-_F4AaA2c-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/gabrielegabrielli/Library/Caches/pypoetry/virtualenvs/neural-networks-_F4AaA2c-py3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 0 : 0.6771374940872192\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.626218, roc auc: 0.7903\n",
      "\n",
      "we are in epoch 0\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 0 : 0.6029232740402222\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.509080, roc auc: 0.8734\n",
      "\n",
      "Test LOSS IMPROVED from 100000.0 to 0.5090796500444412, saving\n",
      "we are in epoch 1\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 1 : 0.5206272602081299\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.415037, roc auc: 0.9172\n",
      "\n",
      "Test LOSS IMPROVED from 0.5090796500444412 to 0.4150373041629791, saving\n",
      "we are in epoch 2\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 2 : 0.42236918210983276\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.368552, roc auc: 0.9272\n",
      "\n",
      "Test LOSS IMPROVED from 0.4150373041629791 to 0.36855174601078033, saving\n",
      "we are in epoch 3\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 3 : 0.3310190737247467\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.358103, roc auc: 0.9213\n",
      "\n",
      "Test LOSS IMPROVED from 0.36855174601078033 to 0.3581029027700424, saving\n",
      "we are in epoch 4\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 4 : 0.32980358600616455\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.306466, roc auc: 0.9431\n",
      "\n",
      "Test LOSS IMPROVED from 0.3581029027700424 to 0.30646583437919617, saving\n",
      "we are in epoch 5\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 5 : 0.2926538586616516\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.292848, roc auc: 0.9457\n",
      "\n",
      "Test LOSS IMPROVED from 0.30646583437919617 to 0.2928479313850403, saving\n",
      "we are in epoch 6\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 6 : 0.23687438666820526\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.277622, roc auc: 0.9500\n",
      "\n",
      "Test LOSS IMPROVED from 0.2928479313850403 to 0.2776220887899399, saving\n",
      "we are in epoch 7\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 7 : 0.26557573676109314\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.236941, roc auc: 0.9663\n",
      "\n",
      "Test LOSS IMPROVED from 0.2776220887899399 to 0.2369408905506134, saving\n",
      "we are in epoch 8\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 8 : 0.26389390230178833\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.238548, roc auc: 0.9629\n",
      "\n",
      "Loss 0.23854786157608032, did NOT improve from 0.2369408905506134 for 0 epochs\n",
      "we are in epoch 9\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 9 : 0.2113945484161377\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.236955, roc auc: 0.9556\n",
      "\n",
      "Loss 0.23695452511310577, did NOT improve from 0.2369408905506134 for 1 epochs\n",
      "we are in epoch 10\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "Mean train loss on epoch 10 : 0.2038058489561081\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n",
      "OK!\n",
      "OK!\n",
      "\n",
      "Test set: Average loss: 0.228996, roc auc: 0.9637\n",
      "\n",
      "Test LOSS IMPROVED from 0.2369408905506134 to 0.2289961576461792, saving\n",
      "we are in epoch 11\n",
      "Setting DEVICE:\n",
      "\t MPS is available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for valid_idx in range(n_groups):\n",
    "\n",
    "    # Print loop number\n",
    "    print('Training fold {}'.format(valid_idx))\n",
    "\n",
    "    # Setting log files\n",
    "    logfile =  model_source_dir+'/{}.fold{}.logfile.txt'.format(model_name, valid_idx)\n",
    "    best_w_path = model_source_dir+'/{}.fold{}.best.pt'.format(model_name, valid_idx)\n",
    "    es_w_path =  model_source_dir+'/{}.fold{}.es.pt'.format(model_name, valid_idx)\n",
    "    \n",
    "    # Data augmentation functions\n",
    "    training_aug = aug_train() \n",
    "    validation_aug = aug_val() \n",
    "    \n",
    "    # Learning Rate setting. This will be modified according to cyclic scheduler\n",
    "    curr_lr = 3e-3 \n",
    "    \n",
    "    # Sampler for the trainer\n",
    "    #train_sampler = torch.utils.data.RandomSampler(DataGenerator(folds_id_val[valid_idx],       # GENERATES DATASET FOR LOADING\n",
    "    #                                                             folds_label_val[valid_idx], \n",
    "    #                                                             validation_aug, train_im_source_dir),\n",
    "    #                                               replacement=True)\n",
    "    #                                               #num_samples=samples_per_epoch) # remove this comment lo limit the number of trainin samples per epoch\n",
    "    \n",
    "\n",
    "    # Loader for the training and the validation\n",
    "    train_loader = torch.utils.data.DataLoader(DataGenerator(folds_id_train[valid_idx], \n",
    "                                                             folds_label_train[valid_idx], \n",
    "                                                             training_aug, train_im_source_dir),\n",
    "                                               pin_memory=False,\n",
    "                                               num_workers=4,\n",
    "                                               batch_size=b_size) \n",
    "                                               #sampler=train_sampler)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(DataGenerator(folds_id_val[valid_idx], \n",
    "                                                       folds_label_val[valid_idx], \n",
    "                                                       validation_aug, train_im_source_dir),\n",
    "                                             pin_memory=False,\n",
    "                                             num_workers=1,\n",
    "                                             batch_size=b_size)\n",
    "    \n",
    "    # Loss function \n",
    "    loss_f = nn.BCELoss() # BINARY CROSS ENTROPY\n",
    "\n",
    "    # Import pretrained model\n",
    "    base_model = pretrainedmodels.resnet34(num_classes=1000, \n",
    "                                           pretrained='imagenet').to(device) \n",
    "    \n",
    "    # Shape the model    \n",
    "    model = Net(base_model, 512).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    # Some layers are freezed for the first iteration, by setting the learning rate to zero\n",
    "    optimizer = optim.SGD([{'params': model.layer0.parameters(), 'lr': 0},\n",
    "                           {'params': model.layer1.parameters(), 'lr': 0},\n",
    "                           {'params': model.layer2.parameters(), 'lr': 0},\n",
    "                           {'params': model.layer3.parameters(), 'lr': 0},\n",
    "                           {'params': model.layer4.parameters(), 'lr': 0},\n",
    "                           {'params': model.classif.parameters()}], lr=0.05, momentum=0.9)\n",
    "    \n",
    "    # First Training procedure\n",
    "    train_loss = train(model= model,\n",
    "                           train_loader= train_loader, \n",
    "                           optimizer= optimizer, \n",
    "                           epoch= 0, \n",
    "                           log_interval= 100, \n",
    "                           loss_f= loss_f, \n",
    "                           #samples_per_epoch= samples_per_epoch,\n",
    "                           scheduler= None,\n",
    "                           device=device)\n",
    "    \n",
    "    # First Test procedure\n",
    "    test_loss, score = test(model= model, \n",
    "                                test_loader= val_loader, \n",
    "                                loss_f= loss_f,\n",
    "                                device = device)\n",
    "    \n",
    "    # Log the loss\n",
    "    write_log(logfile, train_loss, test_loss, score, lr = \"not available\")\n",
    "\n",
    "\n",
    "    # Loop on epochs\n",
    "    # start training with all the layers\n",
    "    # train while validation loss decreases, save model at each improvement of test loss. \n",
    "    # if loss does not decreases for 3 epochs, reload last best model, reduce lr by factor of 2. \n",
    "    # If loss still doesn't decrease for 10 epochs, stop the model. \n",
    "\n",
    "    # Values to monitor the loss trough the loop on epochs\n",
    "    best_score = 0\n",
    "    best_loss = 1e5\n",
    "    idx_stop = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Print current epoch\n",
    "        print(f'we are in epoch {epoch}')\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=curr_lr, momentum=0.9)\n",
    "\n",
    "        # Scheduler for triangular cyclic learning rate\n",
    "        scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=curr_lr, max_lr=3*curr_lr, mode = 'triangular')\n",
    "\n",
    "        # Train procedure\n",
    "        train_loss = train(model= model,\n",
    "                           train_loader= train_loader, \n",
    "                           optimizer= optimizer, \n",
    "                           epoch= epoch, \n",
    "                           log_interval= 100, \n",
    "                           loss_f= loss_f, \n",
    "                           #samples_per_epoch= samples_per_epoch,\n",
    "                           scheduler= scheduler,\n",
    "                           device = device)\n",
    "        \n",
    "        # Test procedure\n",
    "        test_loss, score = test(model= model, \n",
    "                                test_loader= val_loader, \n",
    "                                loss_f= loss_f,\n",
    "                                device = device)\n",
    "        \n",
    "        # Log the loss\n",
    "        write_log(logfile, train_loss, test_loss, score, lr = curr_lr)\n",
    "        \n",
    "        # Case: we get a new minimum loss\n",
    "        if test_loss < best_loss:\n",
    "            print('Test LOSS IMPROVED from {} to {}, saving'.format(best_loss, test_loss))\n",
    "            best_loss = test_loss\n",
    "            # Saving the model\n",
    "            torch.save(model.state_dict(), best_w_path)\n",
    "            idx_stop = 0\n",
    "        # Else:\n",
    "        else:\n",
    "            print('Loss {}, did NOT improve from {} for {} epochs'.format(test_loss, best_loss, idx_stop))\n",
    "            idx_stop += 1\n",
    "\n",
    "\n",
    "        # No improvement for 3 epochs - reduce learning rate\n",
    "        if idx_stop>3:\n",
    "            print('Reducing LR by two and reloading best model')\n",
    "            model.load_state_dict(torch.load(best_w_path))\n",
    "            curr_lr = curr_lr/2\n",
    "\n",
    "        # No improvements for 10 epochs - terminate epoch\n",
    "        if idx_stop>10:\n",
    "            print('Stopping the model')\n",
    "            torch.save(model.state_dict(), es_w_path)\n",
    "\n",
    "    fold_loss.update({valid_idx : test_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-networks-_F4AaA2c-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
