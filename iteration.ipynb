{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('MPS is available')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('No acceleration available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione del dataset di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"datasets/df_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ereditando dalla classe Dataset si ottiene un oggetto iterabile dotato di automatic batching\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx): # Questo membro deve essere sovrascritto (overwrite)\n",
    "        \n",
    "        img_name = self.data.loc[idx, 'id']  #Pesco il nome dalla colonna id\n",
    "        img_path = f\"{self.root_dir}/{img_name}.tif\"  #Creo il path specifico\n",
    "        \n",
    "        # Utilizzo OpenCV per leggere l'immagine\n",
    "        image = cv2.imread(img_path)     # Utilizza OpenCV per leggere l'immagine dal percorso specificato\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)     # Converte il formato dell'immagine da BGR (utilizzato da OpenCV) a RGB\n",
    "\n",
    "        label = int(self.data.loc[idx, 'label'])  # Ottiene l'etichetta (target) dalla colonna label\n",
    "        \n",
    "        # Se passiamo una trasformazione, la applica all'immagine\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array (OpenCV image) to PIL Image\n",
    "    #transforms.Resize(256),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "custom_dataset = CustomImageDataset(dataframe=df_train, root_dir='histopathologic-cancer-detection/train', transform=data_transforms)\n",
    "\n",
    "# Create a DataLoader to iterate through your custom dataset\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definizione del modello\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)  # Esempio di ResNet-18 preaddestrata\n",
    "\n",
    "# Impostare il numero di classi in base al tuo dataset\n",
    "num_classes = len(df_train['label'].unique())\n",
    "\n",
    "# Modifica l'ultimo strato completamente connesso per adattarlo al numero di classi del tuo problema\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Definizione della funzione di perdita e dell'ottimizzatore\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ResNet                                   --\n",
       "├─Conv2d: 1-1                            9,408\n",
       "├─BatchNorm2d: 1-2                       128\n",
       "├─ReLU: 1-3                              --\n",
       "├─MaxPool2d: 1-4                         --\n",
       "├─Sequential: 1-5                        --\n",
       "│    └─BasicBlock: 2-1                   --\n",
       "│    │    └─Conv2d: 3-1                  36,864\n",
       "│    │    └─BatchNorm2d: 3-2             128\n",
       "│    │    └─ReLU: 3-3                    --\n",
       "│    │    └─Conv2d: 3-4                  36,864\n",
       "│    │    └─BatchNorm2d: 3-5             128\n",
       "│    └─BasicBlock: 2-2                   --\n",
       "│    │    └─Conv2d: 3-6                  36,864\n",
       "│    │    └─BatchNorm2d: 3-7             128\n",
       "│    │    └─ReLU: 3-8                    --\n",
       "│    │    └─Conv2d: 3-9                  36,864\n",
       "│    │    └─BatchNorm2d: 3-10            128\n",
       "├─Sequential: 1-6                        --\n",
       "│    └─BasicBlock: 2-3                   --\n",
       "│    │    └─Conv2d: 3-11                 73,728\n",
       "│    │    └─BatchNorm2d: 3-12            256\n",
       "│    │    └─ReLU: 3-13                   --\n",
       "│    │    └─Conv2d: 3-14                 147,456\n",
       "│    │    └─BatchNorm2d: 3-15            256\n",
       "│    │    └─Sequential: 3-16             8,448\n",
       "│    └─BasicBlock: 2-4                   --\n",
       "│    │    └─Conv2d: 3-17                 147,456\n",
       "│    │    └─BatchNorm2d: 3-18            256\n",
       "│    │    └─ReLU: 3-19                   --\n",
       "│    │    └─Conv2d: 3-20                 147,456\n",
       "│    │    └─BatchNorm2d: 3-21            256\n",
       "├─Sequential: 1-7                        --\n",
       "│    └─BasicBlock: 2-5                   --\n",
       "│    │    └─Conv2d: 3-22                 294,912\n",
       "│    │    └─BatchNorm2d: 3-23            512\n",
       "│    │    └─ReLU: 3-24                   --\n",
       "│    │    └─Conv2d: 3-25                 589,824\n",
       "│    │    └─BatchNorm2d: 3-26            512\n",
       "│    │    └─Sequential: 3-27             33,280\n",
       "│    └─BasicBlock: 2-6                   --\n",
       "│    │    └─Conv2d: 3-28                 589,824\n",
       "│    │    └─BatchNorm2d: 3-29            512\n",
       "│    │    └─ReLU: 3-30                   --\n",
       "│    │    └─Conv2d: 3-31                 589,824\n",
       "│    │    └─BatchNorm2d: 3-32            512\n",
       "├─Sequential: 1-8                        --\n",
       "│    └─BasicBlock: 2-7                   --\n",
       "│    │    └─Conv2d: 3-33                 1,179,648\n",
       "│    │    └─BatchNorm2d: 3-34            1,024\n",
       "│    │    └─ReLU: 3-35                   --\n",
       "│    │    └─Conv2d: 3-36                 2,359,296\n",
       "│    │    └─BatchNorm2d: 3-37            1,024\n",
       "│    │    └─Sequential: 3-38             132,096\n",
       "│    └─BasicBlock: 2-8                   --\n",
       "│    │    └─Conv2d: 3-39                 2,359,296\n",
       "│    │    └─BatchNorm2d: 3-40            1,024\n",
       "│    │    └─ReLU: 3-41                   --\n",
       "│    │    └─Conv2d: 3-42                 2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            1,024\n",
       "├─AdaptiveAvgPool2d: 1-9                 --\n",
       "├─Linear: 1-10                           1,026\n",
       "=================================================================\n",
       "Total params: 11,177,538\n",
       "Trainable params: 11,177,538\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "writer.add_graph(model, torch.rand(1, 3, 32, 32))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6174 [07:46<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 0.4519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   0%|          | 0/6174 [07:39<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Loss: 0.4131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   0%|          | 0/6174 [07:36<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Loss: 0.3799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   0%|          | 0/6174 [07:36<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Loss: 0.3598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/6174 [07:34<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Loss: 0.3440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   0%|          | 0/6174 [07:30<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Loss: 0.3345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/6174 [2:24:31<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] Loss: 0.3226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/6174 [5:36:51<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] Loss: 0.3115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/6174 [3:23:38<?, ?batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] Loss: 0.3019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/6174 [09:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] Loss: 0.2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "device = torch.device('mps') #torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as t_bar:\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_saved/2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset ('df_test') similarly to how you loaded the training dataset\n",
    "df_test = pd.read_csv(\"datasets/df_test.csv\")  # Load your test dataset CSV file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create an instance of your custom dataset for the test data\n",
    "custom_test_dataset = CustomImageDataset(dataframe=df_test, root_dir='histopathologic-cancer-detection/train', transform=data_transforms)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_loader = torch.utils.data.DataLoader(custom_test_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle for testing\n",
    "\n",
    "# Assuming you have a trained model 'model'\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create empty lists to store predicted probabilities and true labels\n",
    "predicted_probabilities = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate through the test dataset and get predictions\n",
    "for images, labels in test_loader:\n",
    "    # Move images and labels to the appropriate device\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Forward pass to get outputs/predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        #probabilities = torch.sigmoid(outputs)  # Applying sigmoid to get probabilities\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "\n",
    "    predicted_probabilities.append(predicted.cpu().numpy())\n",
    "    true_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate the predictions and true labels\n",
    "predicted_probabilities = np.concatenate(predicted_probabilities)\n",
    "true_labels = np.concatenate(true_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score on Test Dataset: 0.8517566253849945\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROC AUC score\n",
    "auc_score = roc_auc_score(true_labels, predicted_probabilities)\n",
    "\n",
    "print(f\"ROC AUC Score on Test Dataset: {auc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-cancerdetection-DI779Wwo-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
